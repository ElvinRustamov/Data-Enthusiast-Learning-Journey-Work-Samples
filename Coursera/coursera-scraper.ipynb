{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Define a class named Coursera\n",
    "class Coursera:\n",
    "    # Constructor method to initialize the class attributes\n",
    "    def __init__(self, main_url, header, keyword=None):\n",
    "        self.main_url = main_url  # URL of the main Coursera page\n",
    "        self.header = header      # Headers to be used in HTTP requests\n",
    "        self.keyword = keyword    # Optional keyword for filtering courses\n",
    "\n",
    "    # Method to scrape course URLs from multiple pages\n",
    "    def course_url_scraper(self, last_page):\n",
    "        course_urls_all_page = []  # List to store all course URLs\n",
    "        # Iterate through each page until last_page\n",
    "        for page_num in range(1, last_page + 1):\n",
    "            # Construct URL for the current page\n",
    "            url = self.main_url[:self.main_url.find('page=') + 5] + str(page_num) + self.main_url[self.main_url.find('&sort'):]\n",
    "            # Send GET request to the URL\n",
    "            r = requests.get(url, headers=self.header)\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "            # Extract course URLs from the page\n",
    "            course_urls = [f\"https://www.coursera.org{i.get('href')}\" for i in soup.find('div', class_='cds-9 css-0 cds-11 cds-grid-item cds-56 cds-81').find_all('a')][:12]\n",
    "            \n",
    "            # Append course URLs to the list\n",
    "            course_urls_all_page.extend(course_urls)\n",
    "\n",
    "            # Pause execution for 1 second to avoid overwhelming the server\n",
    "            time.sleep(1)\n",
    "            print(page_num)  # Print current page number for tracking progress\n",
    "\n",
    "        # Write all course URLs to a CSV file\n",
    "        pd.DataFrame({'Course Url': course_urls_all_page}).to_csv(f'CourseUrlsKeyword{self.keyword}.csv', index=False)\n",
    "\n",
    "    # Method to scrape course content from URLs provided in a CSV file\n",
    "    def course_content_scraper(self, path, urls_csv):\n",
    "        # Construct the full path of the CSV file containing course URLs\n",
    "        url_ = path + '/' + urls_csv\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(url_)\n",
    "        # Initialize lists to store course information\n",
    "        titles = []\n",
    "        ratings = []\n",
    "        levels = []\n",
    "        durations = []\n",
    "        schedules = []\n",
    "        reviews = []\n",
    "        what_you_will_learn_list = []\n",
    "        skill_gains = []\n",
    "        modules_list = []\n",
    "        instructors_list = []\n",
    "        offered_by_list = []\n",
    "        keywords = []\n",
    "        urls_courses = []\n",
    "\n",
    "        # Extract the keyword from the CSV filename\n",
    "        k = urls_csv[17:-4]\n",
    "\n",
    "        # Iterate through each course URL in the DataFrame\n",
    "        for u in df['Course Url']:\n",
    "            # Check if the URL contains 'query' (i.e., an internal search page)\n",
    "            if 'query' not in u:\n",
    "                # Send GET request to the course URL\n",
    "                r = requests.get(u, headers=self.header)\n",
    "                # Parse the HTML content\n",
    "                soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "                try:\n",
    "                    # Extract course title\n",
    "                    title = soup.find('h1').text\n",
    "\n",
    "                    # Extract course information such as ratings, levels, durations, schedules, and reviews\n",
    "                    information_1 = soup.find_all('div', class_='cds-119 cds-Typography-base css-h1jogs cds-121')\n",
    "                    r = []\n",
    "                    l = []\n",
    "                    d = []\n",
    "                    s = []\n",
    "\n",
    "                    # Separate different types of information based on their content\n",
    "                    for x in information_1:\n",
    "                        if len(x.text) == 3:\n",
    "                            r.append(x.text)\n",
    "                        elif 'level' in x.text:\n",
    "                            l.append(x.text)\n",
    "                        elif 'hour' in x.text:\n",
    "                            d.append(x.text)\n",
    "                        elif 'schedule' in x.text:\n",
    "                            s.append(x.text)\n",
    "\n",
    "                    # Extract additional information like reviews, what you will learn, and skill gains\n",
    "                    information_2 = [i.text for i in soup.find_all('p', class_='cds-119 cds-Typography-base css-dmxkm1 cds-121')]\n",
    "                    review_ = []\n",
    "                    for e in information_2:\n",
    "                        if 'reviews' in e:\n",
    "                            review_.append(e[1:-1])\n",
    "                            break\n",
    "\n",
    "                    what_you_will_learn = [i.text for i in soup.find('div', class_='css-15ko5n9')]\n",
    "                    skill_gain = [i.text for i in soup.find_all('span', class_='css-1l1jvyr')]\n",
    "\n",
    "                    # Extract module information\n",
    "                    modules = [i.text for i in soup.find_all('h3', class_='cds-119 cds-Typography-base css-h1jogs cds-121')]\n",
    "                    try:\n",
    "                        modules = modules[:modules.index('Instructors')]\n",
    "                    except:\n",
    "                        modules = modules[:modules.index('Instructor')]\n",
    "\n",
    "                    # Extract information about who is offering the course\n",
    "                    offered_by = []\n",
    "                    for x in soup.find_all('div', class_='css-15g7tpu'):\n",
    "                        txt = re.sub('Learn more', '', x.text)\n",
    "                        offered_by.append(txt)\n",
    "\n",
    "                    # Extract instructor information\n",
    "                    instructors = []\n",
    "                    for x in soup.find_all('span', class_='cds-119 cds-Typography-base css-e7lgfl cds-121'):\n",
    "                        if x.text not in offered_by:\n",
    "                            instructors.append(x.text)\n",
    "\n",
    "                    # Remove duplicate instructors\n",
    "                    instructors = list(set(instructors))\n",
    "\n",
    "                    # Pause execution for 1 second to avoid overwhelming the server\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    # Append information to respective lists\n",
    "                    titles.append(title)\n",
    "                    keywords.append(k)\n",
    "                    urls_courses.append(u)\n",
    "\n",
    "                    if len(r) > 0: ratings.append(list(set(r))[0])\n",
    "                    else: ratings.append(np.nan)\n",
    "\n",
    "                    if len(l) > 0: levels.append(list(set(l))[0])\n",
    "                    else: levels.append(np.nan)\n",
    "\n",
    "                    if len(d) > 0: durations.append(list(set(d))[0])\n",
    "                    else: durations.append(np.nan)\n",
    "\n",
    "                    if len(s) > 0: schedules.append(list(set(s))[0])\n",
    "                    else: schedules.append(np.nan)\n",
    "\n",
    "                    if len(review_) > 0: reviews.append(list(set(review_))[0])\n",
    "                    else: reviews.append(np.nan)\n",
    "\n",
    "                    try:\n",
    "                        what_you_will_learn.index(\"What you'll learn\")\n",
    "                        what_you_will_learn_list.append(what_you_will_learn[1])\n",
    "                    except:\n",
    "                        what_you_will_learn_list.append(np.nan)\n",
    "\n",
    "                    skill_gains.append(skill_gain)\n",
    "                    modules_list.append(modules)\n",
    "                    instructors_list.append(instructors)\n",
    "                    offered_by_list.append(offered_by)\n",
    "\n",
    "                    # Print progress counter\n",
    "                    print(len(titles))\n",
    "\n",
    "                    # Write interim results to a CSV file every 100 iterations\n",
    "                    if len(titles) % 100 == 0:\n",
    "                        pd.DataFrame({\n",
    "                            'Course Title': titles,\n",
    "                            'Rating': ratings,\n",
    "                            'Level': levels,\n",
    "                            'Duration': durations,\n",
    "                            'Schedule': schedules,\n",
    "                            'Review': reviews,\n",
    "                            'What you will learn': what_you_will_learn_list,\n",
    "                            'Skill gain': skill_gains,\n",
    "                            'Modules': modules_list,\n",
    "                            'Instructor': instructors_list,\n",
    "                            'Offered By': offered_by_list,\n",
    "                            'Keyword': keywords,\n",
    "                            'Course Url': urls_courses\n",
    "                        }).to_csv('trash.csv', index=False)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        # Write all collected information to a CSV file\n",
    "        pd.DataFrame({\n",
    "            'Course Title': titles,\n",
    "            'Rating': ratings,\n",
    "            'Level': levels,\n",
    "            'Duration': durations,\n",
    "            'Schedule': schedules,\n",
    "            'Review': reviews,\n",
    "            'What you will learn': what_you_will_learn_list,\n",
    "            'Skill gain': skill_gains,\n",
    "            'Modules': modules_list,\n",
    "            'Instructor': instructors_list,\n",
    "            'Offered By': offered_by_list,\n",
    "            'Keyword': keywords,\n",
    "            'Course Url': urls_courses\n",
    "        }).to_csv(f'CoursesAbout{k}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = {\n",
    "    'User-Agent':'Your User Agent'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Scrape courses urls for every keyword.\n",
    "\n",
    "main_url = 'https://www.coursera.org/search?query=Math%20and%20Logic&page=80&sortBy=BEST_MATCH'\n",
    "keyword = 'Math and Logic'\n",
    "Coursera(main_url, header, keyword).course_url_scraper(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract useful information from the courses. folder path in where the courses urls located.\n",
    "\n",
    "folder_path = 'PathWhereUrlsFilesAre'\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "for url in csv_files:\n",
    "    Coursera(url, header).course_content_scraper(folder_path, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Concatenation.\n",
    "\n",
    "# Specify the folder path containing CSV files\n",
    "folder_path = 'PathWhereDataFilesAre'\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty DataFrame to store the merged data\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "# Iterate through each CSV file and merge its data into the main DataFrame\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    merged_data = pd.concat([merged_data, df], ignore_index=True)\n",
    "\n",
    "# Optionally, you can save the merged data to a new CSV file\n",
    "merged_data.to_csv('PathWhereYouWantToCreateThisFile/CourseraDataset.csv', index=False)\n",
    "\n",
    "# Print the merged data\n",
    "print(merged_data)\n",
    "merged_data.duplicated().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
